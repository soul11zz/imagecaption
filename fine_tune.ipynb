{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "374d38b7",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/soul11zz/imagecaption/blob/main/fine_tune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f0694f-82ff-4dfc-9eb3-8a0b3e331ebf",
   "metadata": {},
   "source": [
    "## Train with Pytorch Lighning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3c9f5fd-e585-47c3-9419-0fa3d31b8ff3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "if osp.exists(\"imagecaption\"):\n",
    "    os.chdir(\"imagecaption\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec4460c2-3fee-4e34-8997-d5c43e7ae7f3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers.git (from -r train/requirements.txt (line 1))\n",
      "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-4csdobh4\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-4csdobh4\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit 73a2ff69740123ef85343580cbfa9ee8ce5e6fd5\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pytorch-lightning==1.7.7 in /usr/local/lib/python3.9/dist-packages (from -r train/requirements.txt (line 2)) (1.7.7)\n",
      "Requirement already satisfied: datasets>=2.4.0 in /usr/local/lib/python3.9/dist-packages (from -r train/requirements.txt (line 3)) (2.9.0)\n",
      "Requirement already satisfied: sentencepiece>=0.1.97 in /usr/local/lib/python3.9/dist-packages (from -r train/requirements.txt (line 4)) (0.1.97)\n",
      "Requirement already satisfied: nltk==3.7 in /usr/local/lib/python3.9/dist-packages (from -r train/requirements.txt (line 5)) (3.7)\n",
      "Requirement already satisfied: orjson>=3.6.8 in /usr/local/lib/python3.9/dist-packages (from -r train/requirements.txt (line 6)) (3.8.5)\n",
      "Requirement already satisfied: mlflow==1.25.1 in /usr/local/lib/python3.9/dist-packages (from -r train/requirements.txt (line 7)) (1.25.1)\n",
      "Requirement already satisfied: nvgpu in /usr/local/lib/python3.9/dist-packages (from -r train/requirements.txt (line 8)) (0.9.0)\n",
      "Requirement already satisfied: lightning-bolts in /usr/local/lib/python3.9/dist-packages (from -r train/requirements.txt (line 9)) (0.6.0.post1)\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.9/dist-packages (from -r train/requirements.txt (line 10)) (2.9.1)\n",
      "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.9/dist-packages (from -r train/requirements.txt (line 11)) (2.5.1)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.7.7->-r train/requirements.txt (line 2)) (21.3)\n",
      "Requirement already satisfied: pyDeprecate>=0.3.1 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.7.7->-r train/requirements.txt (line 2)) (0.3.2)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.7.7->-r train/requirements.txt (line 2)) (1.23.1)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.7.7->-r train/requirements.txt (line 2)) (0.11.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.7.7->-r train/requirements.txt (line 2)) (4.3.0)\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.7.7->-r train/requirements.txt (line 2)) (2022.5.0)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.7.7->-r train/requirements.txt (line 2)) (4.64.0)\n",
      "Requirement already satisfied: torch>=1.9.* in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.7.7->-r train/requirements.txt (line 2)) (1.12.0+cu116)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.7.7->-r train/requirements.txt (line 2)) (5.4.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk==3.7->-r train/requirements.txt (line 5)) (8.1.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk==3.7->-r train/requirements.txt (line 5)) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk==3.7->-r train/requirements.txt (line 5)) (2022.7.9)\n",
      "Requirement already satisfied: databricks-cli>=0.8.7 in /usr/local/lib/python3.9/dist-packages (from mlflow==1.25.1->-r train/requirements.txt (line 7)) (0.17.4)\n",
      "Requirement already satisfied: docker>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from mlflow==1.25.1->-r train/requirements.txt (line 7)) (6.0.1)\n",
      "Requirement already satisfied: entrypoints in /usr/local/lib/python3.9/dist-packages (from mlflow==1.25.1->-r train/requirements.txt (line 7)) (0.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from mlflow==1.25.1->-r train/requirements.txt (line 7)) (1.8.1)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.9/dist-packages (from mlflow==1.25.1->-r train/requirements.txt (line 7)) (2.1.0)\n",
      "Requirement already satisfied: Flask in /usr/local/lib/python3.9/dist-packages (from mlflow==1.25.1->-r train/requirements.txt (line 7)) (2.2.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from mlflow==1.25.1->-r train/requirements.txt (line 7)) (1.4.3)\n",
      "Requirement already satisfied: prometheus-flask-exporter in /usr/local/lib/python3.9/dist-packages (from mlflow==1.25.1->-r train/requirements.txt (line 7)) (0.21.0)\n",
      "Requirement already satisfied: protobuf>=3.7.0 in /usr/local/lib/python3.9/dist-packages (from mlflow==1.25.1->-r train/requirements.txt (line 7)) (3.19.4)\n",
      "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.9/dist-packages (from mlflow==1.25.1->-r train/requirements.txt (line 7)) (1.4.39)\n",
      "Requirement already satisfied: gitpython>=2.1.0 in /usr/local/lib/python3.9/dist-packages (from mlflow==1.25.1->-r train/requirements.txt (line 7)) (3.1.27)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.9/dist-packages (from mlflow==1.25.1->-r train/requirements.txt (line 7)) (2022.1)\n",
      "Requirement already satisfied: importlib-metadata!=4.7.0,>=3.7.0 in /usr/local/lib/python3.9/dist-packages (from mlflow==1.25.1->-r train/requirements.txt (line 7)) (4.12.0)\n",
      "Requirement already satisfied: sqlparse>=0.3.1 in /usr/local/lib/python3.9/dist-packages (from mlflow==1.25.1->-r train/requirements.txt (line 7)) (0.4.3)\n",
      "Requirement already satisfied: querystring-parser in /usr/local/lib/python3.9/dist-packages (from mlflow==1.25.1->-r train/requirements.txt (line 7)) (1.2.4)\n",
      "Requirement already satisfied: alembic in /usr/local/lib/python3.9/dist-packages (from mlflow==1.25.1->-r train/requirements.txt (line 7)) (1.9.2)\n",
      "Requirement already satisfied: gunicorn in /usr/local/lib/python3.9/dist-packages (from mlflow==1.25.1->-r train/requirements.txt (line 7)) (20.1.0)\n",
      "Requirement already satisfied: requests>=2.17.3 in /usr/local/lib/python3.9/dist-packages (from mlflow==1.25.1->-r train/requirements.txt (line 7)) (2.28.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.27.0.dev0->-r train/requirements.txt (line 1)) (3.7.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.27.0.dev0->-r train/requirements.txt (line 1)) (0.12.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.27.0.dev0->-r train/requirements.txt (line 1)) (0.12.0)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets>=2.4.0->-r train/requirements.txt (line 3)) (3.0.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.4.0->-r train/requirements.txt (line 3)) (8.0.0)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.4.0->-r train/requirements.txt (line 3)) (0.18.0)\n",
      "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.4.0->-r train/requirements.txt (line 3)) (0.3.5.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets>=2.4.0->-r train/requirements.txt (line 3)) (0.70.13)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets>=2.4.0->-r train/requirements.txt (line 3)) (3.8.1)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.9/dist-packages (from nvgpu->-r train/requirements.txt (line 8)) (0.9.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.9/dist-packages (from nvgpu->-r train/requirements.txt (line 8)) (1.1.0)\n",
      "Requirement already satisfied: pynvml in /usr/local/lib/python3.9/dist-packages (from nvgpu->-r train/requirements.txt (line 8)) (11.4.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from nvgpu->-r train/requirements.txt (line 8)) (5.9.1)\n",
      "Requirement already satisfied: arrow in /usr/local/lib/python3.9/dist-packages (from nvgpu->-r train/requirements.txt (line 8)) (1.2.3)\n",
      "Requirement already satisfied: ansi2html in /usr/local/lib/python3.9/dist-packages (from nvgpu->-r train/requirements.txt (line 8)) (1.8.0)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from nvgpu->-r train/requirements.txt (line 8)) (1.14.0)\n",
      "Requirement already satisfied: flask-restful in /usr/local/lib/python3.9/dist-packages (from nvgpu->-r train/requirements.txt (line 8)) (0.3.9)\n",
      "Requirement already satisfied: torchvision>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from lightning-bolts->-r train/requirements.txt (line 9)) (0.13.0+cu116)\n",
      "Requirement already satisfied: lightning-utilities!=0.4.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from lightning-bolts->-r train/requirements.txt (line 9)) (0.6.0.post0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard->-r train/requirements.txt (line 10)) (2.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard->-r train/requirements.txt (line 10)) (3.3.7)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard->-r train/requirements.txt (line 10)) (63.1.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard->-r train/requirements.txt (line 10)) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard->-r train/requirements.txt (line 10)) (1.8.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.9/dist-packages (from tensorboard->-r train/requirements.txt (line 10)) (0.35.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard->-r train/requirements.txt (line 10)) (2.9.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard->-r train/requirements.txt (line 10)) (0.4.6)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard->-r train/requirements.txt (line 10)) (1.47.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.9/dist-packages (from tensorboard->-r train/requirements.txt (line 10)) (1.1.0)\n",
      "Requirement already satisfied: pyjwt>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from databricks-cli>=0.8.7->mlflow==1.25.1->-r train/requirements.txt (line 7)) (2.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from databricks-cli>=0.8.7->mlflow==1.25.1->-r train/requirements.txt (line 7)) (3.2.0)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.9/dist-packages (from docker>=4.0.0->mlflow==1.25.1->-r train/requirements.txt (line 7)) (0.57.0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.9/dist-packages (from docker>=4.0.0->mlflow==1.25.1->-r train/requirements.txt (line 7)) (1.26.10)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from gitpython>=2.1.0->mlflow==1.25.1->-r train/requirements.txt (line 7)) (4.0.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r train/requirements.txt (line 10)) (5.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r train/requirements.txt (line 10)) (4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r train/requirements.txt (line 10)) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r train/requirements.txt (line 10)) (1.3.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata!=4.7.0,>=3.7.0->mlflow==1.25.1->-r train/requirements.txt (line 7)) (3.8.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging>=17.0->pytorch-lightning==1.7.7->-r train/requirements.txt (line 2)) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.17.3->mlflow==1.25.1->-r train/requirements.txt (line 7)) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.17.3->mlflow==1.25.1->-r train/requirements.txt (line 7)) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.17.3->mlflow==1.25.1->-r train/requirements.txt (line 7)) (2.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision>=0.10.0->lightning-bolts->-r train/requirements.txt (line 9)) (9.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard->-r train/requirements.txt (line 10)) (2.1.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.4.0->-r train/requirements.txt (line 3)) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.4.0->-r train/requirements.txt (line 3)) (1.7.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.4.0->-r train/requirements.txt (line 3)) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.4.0->-r train/requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.4.0->-r train/requirements.txt (line 3)) (18.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.4.0->-r train/requirements.txt (line 3)) (6.0.2)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.9/dist-packages (from alembic->mlflow==1.25.1->-r train/requirements.txt (line 7)) (1.2.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from sqlalchemy->mlflow==1.25.1->-r train/requirements.txt (line 7)) (1.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.9/dist-packages (from arrow->nvgpu->-r train/requirements.txt (line 8)) (2.8.2)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.9/dist-packages (from Flask->mlflow==1.25.1->-r train/requirements.txt (line 7)) (2.1.2)\n",
      "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.9/dist-packages (from Flask->mlflow==1.25.1->-r train/requirements.txt (line 7)) (3.1.2)\n",
      "Requirement already satisfied: aniso8601>=0.82 in /usr/local/lib/python3.9/dist-packages (from flask-restful->nvgpu->-r train/requirements.txt (line 8)) (9.0.1)\n",
      "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.9/dist-packages (from prometheus-flask-exporter->mlflow==1.25.1->-r train/requirements.txt (line 7)) (0.9.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->gitpython>=2.1.0->mlflow==1.25.1->-r train/requirements.txt (line 7)) (5.0.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r train/requirements.txt (line 10)) (0.4.8)\n",
      "Installing collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r train/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff8bd00d-cb33-41c2-bc7b-51d9d9743f13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os.path as osp\n",
    "import sys\n",
    "train_dir = osp.abspath(\"train\")\n",
    "if train_dir not in sys.path:\n",
    "    sys.path.append(train_dir)\n",
    "    \n",
    "import pl_module\n",
    "import pytorch_lightning as pl\n",
    "from huggingface_hub import HfApi, notebook_login\n",
    "\n",
    "pl.seed_everything(42, workers=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b57429",
   "metadata": {},
   "source": [
    "### Login to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbf08f55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa98e7e7ee014cd0a2b5093b1f198a16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826b4dab",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "239fd9e3-688d-4f1a-830a-09e3b04b8d7d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "978ead5f8a754bf88444193fc9dee9fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/554 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-29-05-42-50  WARNING    Using custom data configuration soul11zz--image-caption-desc-only-7581b595eec46650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None to /root/.cache/huggingface/datasets/soul11zz___parquet/soul11zz--image-caption-desc-only-7581b595eec46650/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bc244f920c64b75a270c62a3bfd5ef5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a5f7f2eeb10403481dbb6c682384886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/446M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba0b0e6bf26b400c9d2e60ef8b3285a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/443M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e59442f3d93425ba5eba88063deafaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/442M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77ad0c7e23c845cb895488273cc5c590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/409M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aeaae7680604d969bee0f626d04ff4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/430M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbeb144596eb49ca8ef86bd820c36918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/433M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b584524192a8459e9552016bca28f5ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/428M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e3edd641ea473e94edf3949b06cb3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b35c99a6cdd6479eb1be884668ec033a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/263M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b724333deee2455999f559a421517170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/319M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a88b961697264416959dc12b882892bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/310M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-29-05-44-04  WARNING    Computing checksums of downloaded files. They can be used for integrity verification. You can disable this by passing ignore_verifications=True to load_dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abce285f1be34b54bdfe6ce30cba097b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing checksums:  45%|####5     | 5/11 [00:05<00:06,  1.16s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d97ca19dbbc6465fb56bfc14778a07ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/13065 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/2306 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/2713 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/soul11zz___parquet/soul11zz--image-caption-desc-only-7581b595eec46650/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-29-05-44-30  WARNING    Using custom data configuration soul11zz--image-caption-desc-only-7581b595eec46650\n",
      "2023-01-29-05-44-30  WARNING    Found cached dataset parquet (/root/.cache/huggingface/datasets/soul11zz___parquet/soul11zz--image-caption-desc-only-7581b595eec46650/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "2023-01-29-05-44-30  WARNING    Using custom data configuration soul11zz--image-caption-desc-only-7581b595eec46650\n",
      "2023-01-29-05-44-30  WARNING    Found cached dataset parquet (/root/.cache/huggingface/datasets/soul11zz___parquet/soul11zz--image-caption-desc-only-7581b595eec46650/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81c32dc1753e4d55968d7ec9e18f489a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)rocessor_config.json:   0%|          | 0.00/503 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d20653a85504c2981cafbadfce0906c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/453 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "324f558f6e524b308637b250fe85bd95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7796baa83764cacb8b09918bdd760ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d962026c370642d988ec59d9ea213ba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from pl_module import ImageCaptioningModule\n",
    "from dataset import ImageCaptioningDataset\n",
    "from transformers import GitProcessor, GitForCausalLM\n",
    "\n",
    "from pytorch_lightning.loggers.tensorboard import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "out_model = dataset_desc_only = \"soul11zz/image-caption-desc-only\"\n",
    "in_model = \"microsoft/git-large\"\n",
    "# model_name = get_input_model_name(out_model)\n",
    "\n",
    "dt_train = load_dataset(dataset_desc_only, split=\"train\")\n",
    "dt_val = load_dataset(dataset_desc_only, split=\"validation\")\n",
    "dt_test = load_dataset(dataset_desc_only, split=\"test\")\n",
    "\n",
    "processor = GitProcessor.from_pretrained(in_model)\n",
    "train_dataset = ImageCaptioningDataset(dt_train, processor)\n",
    "val_dataset = ImageCaptioningDataset(dt_val, processor)\n",
    "test_dataset = ImageCaptioningDataset(dt_test, processor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6defd2-5e9c-4cba-b851-eea6471f53c0",
   "metadata": {},
   "source": [
    "### Prelims for PL Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dedab0cf-1881-42c5-9aed-5c0c1cba4d76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "model_dir = \"tmp/model\"\n",
    "epochs = 10\n",
    "\n",
    "if not osp.exists(model_dir):\n",
    "  os.makedirs(model_dir)\n",
    "  \n",
    "num_workers = os.cpu_count() if os.name != \"nt\" else 0\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "model = GitForCausalLM.from_pretrained(in_model)\n",
    "callbacks = []\n",
    "\n",
    "pl_train_module = ImageCaptioningModule(processor, model, train_loader, val_loader, test_loader, learning_rate=1e-2)\n",
    "\n",
    "### Trainer\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"image-captioning\")\n",
    "\n",
    "checkpoint = ModelCheckpoint(dirpath=model_dir, \n",
    "                              save_top_k=2, monitor=\"val_loss\", \n",
    "                              mode=\"min\", \n",
    "                              filename=\"imcap-{epoch:02d}-{val_loss:.2f}\")\n",
    "\n",
    "callbacks += [checkpoint]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123a3fc1",
   "metadata": {},
   "source": [
    "### Create PL Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "905ad3b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer( \n",
    "                      logger=logger, \n",
    "                      gpus=1,\n",
    "                      callbacks=callbacks,\n",
    "                      max_epochs=epochs,\n",
    "                      check_val_every_n_epoch=1,\n",
    "                      # val_check_interval=50,\n",
    "                      precision=16,\n",
    "                      num_sanity_val_steps=1,\n",
    "                      )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b197aba6",
   "metadata": {},
   "source": [
    "### (optional) Tune Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33422c26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 90.00 MiB (GPU 0; 23.69 GiB total capacity; 21.57 GiB already allocated; 14.56 MiB free; 21.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m tuner \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(auto_lr_find\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, devices\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpl_train_module\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py:1052\u001b[0m, in \u001b[0;36mTrainer.tune\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, scale_batch_size_kwargs, lr_find_kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_connector\u001b[38;5;241m.\u001b[39mattach_data(\n\u001b[1;32m   1048\u001b[0m     model, train_dataloaders\u001b[38;5;241m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[38;5;241m=\u001b[39mval_dataloaders, datamodule\u001b[38;5;241m=\u001b[39mdatamodule\n\u001b[1;32m   1049\u001b[0m )\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1052\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tune\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_batch_size_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscale_batch_size_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_find_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_find_kwargs\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtuning \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/tuner/tuning.py:70\u001b[0m, in \u001b[0;36mTuner._tune\u001b[0;34m(self, model, scale_batch_size_kwargs, lr_find_kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mauto_lr_find:\n\u001b[1;32m     69\u001b[0m     lr_find_kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdate_attr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 70\u001b[0m     result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr_find\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mlr_find\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlr_find_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mFINISHED\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/tuner/lr_finder.py:244\u001b[0m, in \u001b[0;36mlr_find\u001b[0;34m(trainer, model, min_lr, max_lr, num_training, mode, early_stop_threshold, update_attr)\u001b[0m\n\u001b[1;32m    241\u001b[0m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39msetup_optimizers \u001b[38;5;241m=\u001b[39m lr_finder\u001b[38;5;241m.\u001b[39m_exchange_scheduler(trainer, model)  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m# Fit, lr & loss logged in callback\u001b[39;00m\n\u001b[0;32m--> 244\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# Prompt if we stopped early\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m!=\u001b[39m num_training:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/tuner/tuning.py:80\u001b[0m, in \u001b[0;36mTuner._run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING  \u001b[38;5;66;03m# last `_run` call might have set it to `FINISHED`\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mtuning \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/trainer.py:1147\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger_connector\u001b[38;5;241m.\u001b[39mreset_metrics()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;66;03m# strategy will configure model and move it to the device\u001b[39;00m\n\u001b[0;32m-> 1147\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[38;5;66;03m# hook\u001b[39;00m\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m==\u001b[39m TrainerFn\u001b[38;5;241m.\u001b[39mFITTING:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/strategies/single_device.py:73\u001b[0m, in \u001b[0;36mSingleDeviceStrategy.setup\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup\u001b[39m(\u001b[38;5;28mself\u001b[39m, trainer: pl\u001b[38;5;241m.\u001b[39mTrainer) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msetup(trainer)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/strategies/single_device.py:70\u001b[0m, in \u001b[0;36mSingleDeviceStrategy.model_to_device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodel_to_device\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself.model must be set before self.model.to()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot_device\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorch_lightning/core/mixins/device_dtype_mixin.py:113\u001b[0m, in \u001b[0;36mDeviceDtypeModuleMixin.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39m_parse_to(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__update_properties(device\u001b[38;5;241m=\u001b[39mout[\u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mout[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    924\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 927\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 579 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:602\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 602\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:925\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m    923\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    924\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 925\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 0; 23.69 GiB total capacity; 21.57 GiB already allocated; 14.56 MiB free; 21.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "tuner = pl.Trainer(auto_lr_find=True, accelerator=\"cuda\", devices=1, max_epochs=1)\n",
    "tuner.tune(pl_train_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1689e83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-27T01:37:29.606963Z",
     "iopub.status.busy": "2023-01-27T01:37:29.606308Z",
     "iopub.status.idle": "2023-01-27T01:37:29.611542Z",
     "shell.execute_reply": "2023-01-27T01:37:29.610880Z",
     "shell.execute_reply.started": "2023-01-27T01:37:29.606940Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl_train_module.lr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2256ac01",
   "metadata": {},
   "source": [
    "### Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4da1e6",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-01-27T01:37:33.831267Z",
     "iopub.status.busy": "2023-01-27T01:37:33.830472Z",
     "iopub.status.idle": "2023-01-27T02:21:54.056781Z",
     "shell.execute_reply": "2023-01-27T02:21:54.056167Z",
     "shell.execute_reply.started": "2023-01-27T01:37:33.831243Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "trainer.fit(pl_train_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598c756f-8817-4a1b-9cda-db254ed69592",
   "metadata": {},
   "source": [
    "### Upload Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ac42dfa-9325-41c2-ac07-21bf507a3aa2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-27T02:43:24.497998Z",
     "iopub.status.busy": "2023-01-27T02:43:24.497370Z",
     "iopub.status.idle": "2023-01-27T02:43:42.016340Z",
     "shell.execute_reply": "2023-01-27T02:43:42.015693Z",
     "shell.execute_reply.started": "2023-01-27T02:43:24.497977Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a37bd0075445958d6b57c7436798ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/707M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d9baf1d4ff2409d977f8ffae441698d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pl_model_best = ImageCaptioningModule.load_from_checkpoint(checkpoint.best_model_path, processor=processor, model=model, train_dataloader=train_loader, val_dataloader=val_loader)\n",
    "pl_model_best.model.save_pretrained(\"tb_logs/image-captioning/best_model\", push_to_hub=True, repo_id=out_model)\n",
    "pl_model_best.processor.save_pretrained(\"tb_logs/image-captioning/best_model\", push_to_hub=True, repo_id=out_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae97a1d",
   "metadata": {},
   "source": [
    "### Evaluate Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56482e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(pl_train_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1060a6ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "78bfdcee45ab360ff98997b0fdb262d833775071286b5e4a7c2a2c9a5871b9ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
